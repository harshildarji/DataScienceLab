## Cohen's kappa
***Cohenâ€™s kappa*** statistic measures ***interrater reliability***
which is more robust than a simple percent agreement
calculation.

| Annotator 1 | Annotator 2 | Agreement |
|---|---|---|
| Harshil | Abhishek | 0.7467 |
| Harshil | Dilip | 0.7723 |
| Harshil | Arsal | 0.8577 |
| Abhishek | Dilip | 0.7409 |
| Abhishek | Arsal | 0.8500 |
| Dilip | Arsal | 0.8516 |

Mean: **0.8032**